---
title: "XGBoost"
---

# XGBoost
XGBoost is a machine learning technique for both regression and classification problems. It consists of an ensemble of sequential simple decision trees to generalize the final result of them by a LOSS function. 

This method gained in popularity the last few years, being the most used method along with the LightGBM technique.

There are a lot of things to take into consideration when tunning this model. While bagging techniques create weak decision trees and then they calculate the average between them, when using a boosting technique like XGBoost around 5 nodes of decision trees are created and then the algorithm calculates a rolling average from one to the following. One of the pros of this is that the learning curve is smoother compared to bagging models, so the model can steadily learn from the errors, trying at every loop to minimize the MSE LOSS function, and thus avoiding overfitting more efficiently. 

```{r libraries, message=FALSE, warning=FALSE}
# Load required libraries
library(tidyverse)
library(data.table)
library(magrittr)
library(ggridges)
library(xgboost)
library(rpart)
library(gridExtra)
library(vtreat)

# Load databases
train <- fread('/Users/hp/Documents/GitHub/dvalls/archives/titanic_train.csv')
test <- fread('/Users/hp/Documents/GitHub/dvalls/archives/titanic_test.csv')
```

# Exploratory Analysis
```{r}
# Let's predict survivors 
# Surviving Rate By Sex
train %>% 
  group_by(Sex) %>% 
  summarise(total = n(),
            survived = sum(Survived),
            non_survived = sum(ifelse(Survived == 1, 0, 1))) %>%
  gather(key, num, survived, non_survived) %>% 
  mutate(prop = num / total) %>% 
  ggplot(aes(x = Sex, y = num, fill = key)) +
    geom_col(position='fill') +
    geom_label(aes(label = paste(num, '-', round(prop * 100, 2), '%')),
                   position = 'fill') +
    labs(x = 'Sex', y = 'Proportion', fill = '') +
    theme_minimal() +
    theme(axis.text = element_text(size = 12),
          plot.title = element_text(size = 20),
          legend.text = element_text(size = 12)) +
    scale_y_continuous(breaks = c(0, .25, .5, .75, 1),
                       labels = c('0%', '25%', '50%', '75%', '100%')) +
    scale_fill_discrete(labels = c("Non Survived", "Survived")) +
    ggtitle('Surviving Rate by Sex')
```

```{r}
# Surviving by Fare
train %>% 
  filter(Fare < 300) %>% 
  ggplot(aes(x = Fare)) +
    geom_density(aes(fill = factor(Survived)), alpha = .5, size = 1.2) +
    theme_bw() +
    theme(axis.text = element_text(size = 12),
          plot.title = element_text(size = 20),
          legend.title = element_blank(),
          legend.text = element_text(size = 12)) +
    labs(x = 'Fare', y = 'Density') +
    scale_fill_discrete(labels = c("Non Survived", "Survived")) +
    ggtitle('Surviving Rate by Fare')
```

```{r}
# Feature  engineering
# Combining test and train datasets 
combi <- rbind(train, test)

# 1. Choose interesting variables
train2 <- train %>% 
            select(Survived, Pclass, Sex, Age = Age_wiki, Fare, SibSp, Parch)

# 2. Input missing Age using a Decision Tree
fit_age <- rpart(Age ~ SibSp + Parch + Pclass, data = train2)
train2$Age[is.na(train2$Age)] <- predict(fit_age, newdata = train2[is.na(train2$Age),])

# 3. Add a variable for family size
train2 %<>% 
  mutate(family_size = SibSp + Parch + 1) %>% 
  select(-SibSp, -Parch)

train3 <- train2 %>% 
            select(Survived, Pclass, Age, Fare, family_size) 
```

```{r}
# Surviving by Fare
train %>% 
  filter(Fare < 300) %>% 
  ggplot(aes(x = Fare)) +
    geom_density(aes(fill = factor(Survived)), alpha = .5, size = 1.2) +
    theme_bw() +
    theme(axis.text = element_text(size = 12),
          plot.title = element_text(size = 20),
          legend.title = element_blank(),
          legend.text = element_text(size = 12)) +
    labs(x = 'Fare', y = 'Density') +
    scale_fill_discrete(labels = c("Non Survived", "Survived")) +
    ggtitle('Surviving Rate by Fare')
```

# Model fit

```{r}
set.seed(123)

# Create the tunning grid
hyper_grid <- expand.grid(
  eta = c(.05, .1, .2, .3),
  gamma = c(.5, 1, 1.2, 1.5),
  max_depth = c(3, 5, 7, 9),
  min_child_weight = c(1, 3, 5, 7),
  subsample = c(.5, .65, .8, 1),
  colsample_bytree = c(.7, .8, .9, 1),
  #optimal_trees = 0, 
  test_error_mean = 0
)
nrow(hyper_grid)
```
There are 4096 possible model combinations. That's gonna take a bit to run... 

```{r eval=FALSE, include=FALSE}
# 3. For loop (It took me 1 hour to run it!)
# grid search 
for (i in 1:nrow(hyper_grid)) {
  params <- list(
    eta = hyper_grid$eta[i],
    gamma = hyper_grid$gamma[i], 
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i]
  )
  xgb.tune <- xgb.cv(
    params = params,
    data = data.matrix(train3[,-1]),
    label = train3[,1],
    nrounds = 500,
    nfold = 5,
    objective = 'binary:logistic',  # classificatory model
    verbose = 0,
    early_stopping_rounds = 10      # 
  )
  # add min training error and trees to grid
  #hyper_grid$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
  hyper_grid$test_error_mean[i] <- min(xgb.tune$evaluation_log$test_error_mean)
}
```

That took well over 1 hour to run on my laptop. Now let's take a look at the best parameter combination to fit our predictive model.

```{r}
hyper_grid %>%
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

```{r}
# Let's pick up the best model
# Definitive model
params_final <- list(
  eta = 0.3,
  gamma = 1.2, 
  max_depth = 3, 
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = .9
)

xgb.fit <- xgb.cv(
  params = params_final,
  data = data.matrix(train3[,-1]),
  label = train3[,1],
  nrounds = 500,
  nfold = 5,
  objective = 'binary:logistic',
  #verbose = 0,
  early_stopping_rounds = 10,
  print_every_n = 100
)

xgb.fit <- xgboost(
  params = params_final,
  data = data.matrix(train3[,-1]),
  label = train3[,1],
  nrounds = 500,
  nfold = 5,
  objective = 'binary:logistic',
  #verbose = 0,
  early_stopping_rounds = 10,
  print_every_n = 100
)
# Test error: 0.24 

```
